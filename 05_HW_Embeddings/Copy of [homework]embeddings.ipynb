{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of [homework]embeddings.ipynb","provenance":[{"file_id":"1xhTjtV71TLnQCic42j_1RVGt65FOU7Qy","timestamp":1614192948043}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8e295c0b9e3844879c528a8c6359e4b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b93c5306a4964fa8b165fc2e514db46f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dacbef164ecc41b1b1e0a5cb56f7af7d","IPY_MODEL_7d84d719d2f940e086ec78e5249fdf00"]}},"b93c5306a4964fa8b165fc2e514db46f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dacbef164ecc41b1b1e0a5cb56f7af7d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1a180c82af9b4ded8da072dd97cd53bf","_dom_classes":[],"description":"Epoch 1. Train Loss: 0.4817: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_07f707f851b54e77afa974c9a5a20105"}},"7d84d719d2f940e086ec78e5249fdf00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_17ffae7a77c94c26b61282ae1851051c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [17:54&lt;00:00,  1.07s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_592c3bbacf6b42328800b872bfc7f549"}},"1a180c82af9b4ded8da072dd97cd53bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"07f707f851b54e77afa974c9a5a20105":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17ffae7a77c94c26b61282ae1851051c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"592c3bbacf6b42328800b872bfc7f549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a46023a1450d4a59b56908bcb1fce7e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6e5fe77f909b4438986cd2b1915558ec","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2bbf5deeadf643fe9a472b8933fb5239","IPY_MODEL_5acd8c25a09f4700bef9c47a0ee27322"]}},"6e5fe77f909b4438986cd2b1915558ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2bbf5deeadf643fe9a472b8933fb5239":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_562c0451215f4cc3975de65e86ffc1fc","_dom_classes":[],"description":"Test Loss: 0.4978, Test Acc: 0.7554: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":250,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_621bb5ec512d439c96e283747b1309bf"}},"5acd8c25a09f4700bef9c47a0ee27322":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1d7354207c0a4d139c874caf309ec188","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 250/250 [15:02&lt;00:00,  3.61s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_33e4100708724aae8fe71de2c9a358cb"}},"562c0451215f4cc3975de65e86ffc1fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"621bb5ec512d439c96e283747b1309bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d7354207c0a4d139c874caf309ec188":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"33e4100708724aae8fe71de2c9a358cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ba175355a7f9484fb491cc363e6220ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_93d9109354344fb1921f8f3614251107","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_39057135209f4d92b2762235ea19b6a6","IPY_MODEL_c7dba86c70e44fd2a8db2bda9ba92d21"]}},"93d9109354344fb1921f8f3614251107":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39057135209f4d92b2762235ea19b6a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c86f22e37d844852b7b7c75ef10ed377","_dom_classes":[],"description":"Epoch 2. Train Loss: 0.4552: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0be6e5e828d422c981372da7094082e"}},"c7dba86c70e44fd2a8db2bda9ba92d21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4a8ac5c2de864580b264a99926a9c171","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [14:17&lt;00:00,  1.17it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_db004ee4ac1d495cb48e55abffffdfdf"}},"c86f22e37d844852b7b7c75ef10ed377":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b0be6e5e828d422c981372da7094082e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a8ac5c2de864580b264a99926a9c171":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"db004ee4ac1d495cb48e55abffffdfdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43a57e094a4b4533be9c624ac2e133b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a37e1f570c1d49d5a834775c3bbc3643","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_062fd71a299e4a02b947c438d57f535b","IPY_MODEL_f12c3f51631e4c23950682ec2aea5813"]}},"a37e1f570c1d49d5a834775c3bbc3643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"062fd71a299e4a02b947c438d57f535b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_729e0169b51446b685b577d1ad728291","_dom_classes":[],"description":"Test Loss: 0.4952, Test Acc: 0.7572: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":250,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_84068874d3ea448184fa230ed9ae3c35"}},"f12c3f51631e4c23950682ec2aea5813":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_156ac608c2b44d789f219fa46b6a4b19","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 250/250 [11:27&lt;00:00,  2.75s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c854291b49ce45c19b362cac44d6e704"}},"729e0169b51446b685b577d1ad728291":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"84068874d3ea448184fa230ed9ae3c35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"156ac608c2b44d789f219fa46b6a4b19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c854291b49ce45c19b362cac44d6e704":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aadfe725f8f34222a60dc67a2a9395ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4b7989ca9482430296813ce1132f8f32","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b4ff246a72b74576bff9745210df03bf","IPY_MODEL_7bb408a7361042698d4778e127daa7bd"]}},"4b7989ca9482430296813ce1132f8f32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4ff246a72b74576bff9745210df03bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_98a1f7ccc8de45a492947a90e8422c7d","_dom_classes":[],"description":"Epoch 3. Train Loss: 0.5183: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5eafab15d51242eda9d8cb4fc49a649a"}},"7bb408a7361042698d4778e127daa7bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_713517703bc74312939d5e84ebed8aac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [10:43&lt;00:00,  1.55it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_152cb86d1cce4435ba05bb5026be5f33"}},"98a1f7ccc8de45a492947a90e8422c7d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5eafab15d51242eda9d8cb4fc49a649a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"713517703bc74312939d5e84ebed8aac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"152cb86d1cce4435ba05bb5026be5f33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2b896ff9b514519bf240a2bff55dd3e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_da1215fd93774f6b9ba6d25edfb4b1b9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_648e8998d7754f1494580cd1d01bf9f0","IPY_MODEL_7d8a11b8f2b84a05ab066ab50e9ff991"]}},"da1215fd93774f6b9ba6d25edfb4b1b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"648e8998d7754f1494580cd1d01bf9f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c7d5db1b5dbd47329852406c1282fe66","_dom_classes":[],"description":"Test Loss: 0.4909, Test Acc: 0.7601: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":250,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45dce49739b848ef8c0e3fd62074b012"}},"7d8a11b8f2b84a05ab066ab50e9ff991":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_86b44ccc9d384f83b1d563211f115608","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 250/250 [00:55&lt;00:00,  4.49it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3283713c9ef34a50961d40442f5f5e92"}},"c7d5db1b5dbd47329852406c1282fe66":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"45dce49739b848ef8c0e3fd62074b012":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"86b44ccc9d384f83b1d563211f115608":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3283713c9ef34a50961d40442f5f5e92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c239c5842362430f954119b567d96814":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_257c9991bd3546c4ae429500f738bea0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a2cd9ef6780a4cb98635a060aaa878f5","IPY_MODEL_e9aa6eeea4c94242959743730460314f"]}},"257c9991bd3546c4ae429500f738bea0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2cd9ef6780a4cb98635a060aaa878f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bd2074d768e54428b860fe64f50d055c","_dom_classes":[],"description":"Epoch 4. Train Loss: 0.4849: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79ac6e0f7b68469f9180ec81ce34765c"}},"e9aa6eeea4c94242959743730460314f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3693940645014511bccf75c419a9acb5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [07:11&lt;00:00,  2.32it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b04dbc1ceb094be18192cfbe47650a8d"}},"bd2074d768e54428b860fe64f50d055c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"79ac6e0f7b68469f9180ec81ce34765c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3693940645014511bccf75c419a9acb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b04dbc1ceb094be18192cfbe47650a8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f1816759bb846299f41c079956f5e41":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3409675434a1415a8a400ca9c8ca8ba8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3a6d4838428a448e834a4b9088674848","IPY_MODEL_f098d977d54e405a8def86853d5b8a68"]}},"3409675434a1415a8a400ca9c8ca8ba8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a6d4838428a448e834a4b9088674848":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_416e33fa5c83486598cbeef41c27e303","_dom_classes":[],"description":"Test Loss: 0.4915, Test Acc: 0.7597: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":250,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dcc42c0117f240249fa40303dc1b00ef"}},"f098d977d54e405a8def86853d5b8a68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_19a629f2cc9b463eac0e66be709b705f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 250/250 [00:48&lt;00:00,  5.16it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bc58db755ef34d2c93b8c2404e8a1e29"}},"416e33fa5c83486598cbeef41c27e303":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dcc42c0117f240249fa40303dc1b00ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19a629f2cc9b463eac0e66be709b705f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bc58db755ef34d2c93b8c2404e8a1e29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16c7fe19129f47b28fb33221ae686625":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d7f78e3f5b074afc83f2c5b910b8fb39","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_774a187f9d9c4fb18e8503eba113d996","IPY_MODEL_68de190b024d4bed9ff8ec80a18e30ff"]}},"d7f78e3f5b074afc83f2c5b910b8fb39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"774a187f9d9c4fb18e8503eba113d996":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_39428f9bbb5d4f549d63f620ca28ef5d","_dom_classes":[],"description":"Epoch 5. Train Loss: 0.4691: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7067b8b7397848b3bb0c004fcec49e12"}},"68de190b024d4bed9ff8ec80a18e30ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f0220a7733ea48d980e1e45d35725abc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [03:35&lt;00:00,  4.64it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7fdb53ba64734d8a97c0bcd3fee0471b"}},"39428f9bbb5d4f549d63f620ca28ef5d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7067b8b7397848b3bb0c004fcec49e12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0220a7733ea48d980e1e45d35725abc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7fdb53ba64734d8a97c0bcd3fee0471b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e39c492cf11b4f1abbfec062183658e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bf71ebeb18be4b1a98800d0489a35c66","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ece70eca441c4a08bbf0fdd1eabdf078","IPY_MODEL_66cdf539a9e74648a7553c929ff7d986"]}},"bf71ebeb18be4b1a98800d0489a35c66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ece70eca441c4a08bbf0fdd1eabdf078":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4754e9d7dadf4ac8a1084e5ef35b13b5","_dom_classes":[],"description":"Test Loss: 0.5126, Test Acc: 0.7432: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":250,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca19976432c74208b6494779269d4711"}},"66cdf539a9e74648a7553c929ff7d986":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_52ce62a7cca7460cbd235f2d87b03e2e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 250/250 [00:44&lt;00:00,  5.63it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5306f7aee9854b5e952f8337241a7e18"}},"4754e9d7dadf4ac8a1084e5ef35b13b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ca19976432c74208b6494779269d4711":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"52ce62a7cca7460cbd235f2d87b03e2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5306f7aee9854b5e952f8337241a7e18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a9ebc15fed045fcb805b6d25beccbd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bcec1c1e246c445eb85a782523963b1c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_13c57c146d9b4a11a120d5369e5c0c15","IPY_MODEL_425419dd455c48019902361d482461d0"]}},"bcec1c1e246c445eb85a782523963b1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"13c57c146d9b4a11a120d5369e5c0c15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b0f4ce7dd1345359740e1f4a9870080","_dom_classes":[],"description":"Test Loss: 0.4679, Test Acc: 0.7598: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":313,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":313,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c7166d2e8e9446828d92b871ef324f11"}},"425419dd455c48019902361d482461d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_96570a115c3d46b681eed87c7e911b0c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 313/313 [00:51&lt;00:00,  6.08it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_47c3c4d95ee549adb62d1c700c5311b2"}},"4b0f4ce7dd1345359740e1f4a9870080":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c7166d2e8e9446828d92b871ef324f11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"96570a115c3d46b681eed87c7e911b0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"47c3c4d95ee549adb62d1c700c5311b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JArK-AQcnGM5"},"source":["# Does not work\n","# https://towardsdatascience.com/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-efa636e646ff\n","\n","# a = [i for i in range(10**10)]\n","# a[:20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"fyylXxooH-Gk","executionInfo":{"status":"ok","timestamp":1615112946000,"user_tz":0,"elapsed":16678,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"3d0de0f0-7924-4f1a-fdd1-0983eaf39efa"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"bKZoAKumIFfy","executionInfo":{"status":"ok","timestamp":1615117473870,"user_tz":0,"elapsed":354,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"20c7f15d-28ec-4e86-b532-bee06bb56620"},"source":["%cd drive/MyDrive/colab_projects/nlp_from_dls/5_HW_Embeddings/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/colab_projects/nlp_from_dls/5_HW_Embeddings\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"_PZgWd1aIlX_","executionInfo":{"status":"ok","timestamp":1615117475284,"user_tz":0,"elapsed":530,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"e7bf399b-a06e-4467-8514-105a3221d1ce"},"source":["%ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":[" archive.zip\n","'Copy of [homework]embeddings.ipynb'\n"," dev_word2tfidf.pkl\n"," model_baseline.pt\n"," model_contex_existing_11.pt\n"," model_context_11_window3_drop_zero.pt\n"," model_tfidf.pt\n"," pca_baseline.png\n"," pca_context_existing_11.png\n"," pca_context_naive.png\n"," test_results_baseline.png\n"," test_results_context_existing_11.png\n"," test_results_context_naive.png\n"," test_word2tfidf.pkl\n"," training.1600000.processed.noemoticon.csv\n"," training_baseline.png\n"," training_context_existing_11.png\n"," training_context_naive.png\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ot3c4fjZwC4T"},"source":["<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"P2JdzEXmwRU5"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"eYtJxkhKpYK2"},"source":["# Embeddings\n","\n","Привет! В этом домашнем задании мы с помощью эмбеддингов решим задачу семантической классификации твитов.\n","\n","Для этого мы воспользуемся предобученными эмбеддингами word2vec."]},{"cell_type":"code","metadata":{"id":"sY3lUQCbZauV","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1615117484018,"user_tz":0,"elapsed":1142,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"c09609be-2c6a-4af9-f46d-d4120f5727a8"},"source":["%ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":[" archive.zip\n","'Copy of [homework]embeddings.ipynb'\n"," dev_word2tfidf.pkl\n"," model_baseline.pt\n"," model_contex_existing_11.pt\n"," model_context_11_window3_drop_zero.pt\n"," model_tfidf.pt\n"," pca_baseline.png\n"," pca_context_existing_11.png\n"," pca_context_naive.png\n"," test_results_baseline.png\n"," test_results_context_existing_11.png\n"," test_results_context_naive.png\n"," test_word2tfidf.pkl\n"," training.1600000.processed.noemoticon.csv\n"," training_baseline.png\n"," training_context_existing_11.png\n"," training_context_naive.png\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jBOdoFS8AdpP"},"source":["Для начала скачаем датасет для семантической классификации твитов:"]},{"cell_type":"code","metadata":{"id":"wXjhtsfF_gBK","executionInfo":{"status":"ok","timestamp":1615117484354,"user_tz":0,"elapsed":622,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["# !gdown https://drive.google.com/uc?id=1eE1FiUkXkcbw0McId4i7qY-L8hH-_Qph&export=download\n","# !unzip archive.zip"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sh6wW-K53Mle"},"source":["Импортируем нужные библиотеки:"]},{"cell_type":"code","metadata":{"id":"A2Y5CHRm6NFe","executionInfo":{"status":"ok","timestamp":1615117494022,"user_tz":0,"elapsed":6243,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["import math\n","import random\n","import string\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","import torch\n","import nltk\n","import gensim\n","import gensim.downloader as api"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"73Lb0wbESrgQ","executionInfo":{"status":"ok","timestamp":1615117494023,"user_tz":0,"elapsed":4496,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["random.seed(42)\n","np.random.seed(42)\n","torch.random.manual_seed(42)\n","torch.cuda.random.manual_seed(42)\n","torch.cuda.random.manual_seed_all(42)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_Wv-4bu83Fl","executionInfo":{"status":"ok","timestamp":1615117502384,"user_tz":0,"elapsed":12493,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin\",\n","                   header=None, names=[\"emotion\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RY1pvYDS3Yuj"},"source":["Посмотрим на данные:"]},{"cell_type":"code","metadata":{"id":"jST2tjgjCTWD","colab":{"base_uri":"https://localhost:8080/","height":285},"executionInfo":{"status":"ok","timestamp":1615113714636,"user_tz":0,"elapsed":2855,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"947ed154-3a2a-4eb8-9a21-c1c279d8e7be"},"source":["data.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>emotion</th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>flag</th>\n","      <th>user</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1467810369</td>\n","      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>_TheSpecialOne_</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1467810672</td>\n","      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>scotthamilton</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1467810917</td>\n","      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mattycus</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1467811184</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>ElleCTF</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1467811193</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Karoli</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   emotion  ...                                               text\n","0        0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1        0  ...  is upset that he can't update his Facebook by ...\n","2        0  ...  @Kenichan I dived many times for the ball. Man...\n","3        0  ...    my whole body feels itchy and like its on fire \n","4        0  ...  @nationwideclass no, it's not behaving at all....\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"OhbR5JJyA2VW"},"source":["Выведем несколько примеров твитов, чтобы понимать, с чем мы имеем дело:"]},{"cell_type":"code","metadata":{"id":"kCBwe0wR83C2"},"source":["examples = data[\"text\"].sample(10)\n","print(\"\\n\\n\".join(examples))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvcYW8aX3mKt"},"source":["Как вилим, тексты твитов очень \"грязные\". Нужно предобработать датасет, прежде чем строить для него модель классификации.\n","\n","Чтобы сравнивать различные методы обработки текста/модели/прочее, разделим датасет на dev(для обучения модели) и test(для получения качества модели)."]},{"cell_type":"code","metadata":{"id":"f8hUK-jnQg6O"},"source":["indexes = np.arange(data.shape[0])\n","np.random.shuffle(indexes)\n","dev_size = math.ceil(data.shape[0] * 0.8)\n","\n","dev_indexes = indexes[:dev_size]\n","test_indexes = indexes[dev_size:]\n","\n","dev_data = data.iloc[dev_indexes]\n","test_data = data.iloc[test_indexes]\n","\n","dev_data.reset_index(drop=True, inplace=True)\n","test_data.reset_index(drop=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ivcpeFoCnZA"},"source":["## Обработка текста"]},{"cell_type":"markdown","metadata":{"id":"Df4nca285Dar"},"source":["Токенизируем текст, избавимся от знаков пунктуации и выкинем все слова, состоящие менее чем из 4 букв:"]},{"cell_type":"code","metadata":{"id":"nsNHNDES9ZVF"},"source":["tokenizer = nltk.WordPunctTokenizer()\n","line = tokenizer.tokenize(dev_data[\"text\"][0].lower())\n","print('Original line:', dev_data[\"text\"][0])\n","print('Tokens:       ', line)\n","print('Joined tokens:', \" \".join(line))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcBS_u_hTuxp"},"source":["filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n","print(\" \".join(filtered_line))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cuFmlXkC6E7X"},"source":["Загрузим предобученную модель эмбеддингов. \n","\n","Если хотите, можно попробовать другую. Полный список можно найти здесь: https://github.com/RaRe-Technologies/gensim-data.\n","\n","Данная модель выдает эмбеддинги для **слов**. Строить по эмбеддингам слов эмбеддинги предложений мы будем ниже."]},{"cell_type":"code","metadata":{"id":"cACJpje2T5bc","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1615117614061,"user_tz":0,"elapsed":111668,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"19fa1cfa-00e8-4dc8-e6cc-78a20281d69e"},"source":["%time word2vec = api.load(\"word2vec-google-news-300\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["CPU times: user 1min 47s, sys: 3.22 s, total: 1min 50s\n","Wall time: 1min 51s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"5TL731OmHV9_","executionInfo":{"status":"ok","timestamp":1615113929112,"user_tz":0,"elapsed":110441,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"5cf4a2e0-6578-476c-aa68-e310c8cd517b"},"source":["word2vec.vectors.shape"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000000, 300)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"_lpDMb7CHsi_"},"source":["word2vec['dog'].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGm1N6fsSWU8"},"source":["word2vec.vector_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J3xRHE8dHgyu"},"source":["У нас есть эмбединги для 3 миллионов слов. Для каждого слова - 300 фичей."]},{"cell_type":"code","metadata":{"id":"NafmYHrkT5YD"},"source":["emb_line = [word2vec.get_vector(w) for w in filtered_line if w in word2vec]\n","\n","print('Num words from line in the embedding:', len(emb_line))\n","print('Shape of the features:', sum(emb_line).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LTS6LCkd6_E7"},"source":["Нормализуем эмбеддинги, прежде чем обучать на них сеть. \n","(наверное, вы помните, что нейронные сети гораздо лучше обучаются на нормализованных данных)"]},{"cell_type":"code","metadata":{"id":"7Gpd1fRZIdST"},"source":["# not all the words are in the 3*10^6 corpus, we need to filter out the missing ones\n","\n","try:\n","    print(word2vec['russian'].shape)\n","    print(80*'-')\n","    print(word2vec['rassian'].shape)\n","except KeyError as e:\n","    print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PyLTZ6xf3Oq","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1615117619192,"user_tz":0,"elapsed":5129,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"ca81b28f-c7c1-44eb-d44b-27de14cfb8b8"},"source":["%%time\n","\n","# axis=0 - we want mean and standard deviation of each of the 300 features for every word\n","mean = np.mean(word2vec.vectors, axis=0)\n","std = np.std(word2vec.vectors, axis=0)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["CPU times: user 2.25 s, sys: 2.17 s, total: 4.42 s\n","Wall time: 4.45 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ljT894_KKV93"},"source":["norm_emb_line = [(word2vec.get_vector(w) - mean) / std for w in filtered_line if w in word2vec and len(w) > 3]\n","print(sum(norm_emb_line).shape)\n","print([all(norm_emb_line[i] == emb_line[i]) for i in range(len(emb_line))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXFr1n3SXO7S"},"source":["for w in filtered_line:\n","    if w in word2vec:\n","        print(((word2vec.get_vector(w) - mean) / std).shape)\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7vm6Ppd7Ubw"},"source":["Сделаем датасет, который будет по запросу возвращать подготовленные данные."]},{"cell_type":"code","metadata":{"id":"V9pKD_elW_sh","executionInfo":{"status":"ok","timestamp":1615117661592,"user_tz":0,"elapsed":680,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["from torch.utils.data import Dataset, random_split"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4eZajF7pZ1X","executionInfo":{"status":"ok","timestamp":1615117661593,"user_tz":0,"elapsed":580,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["class TwitterDataset(Dataset):\n","    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n","        self.tokenizer = nltk.WordPunctTokenizer()\n","        \n","        self.data = data\n","\n","        self.feature_column = feature_column # e.g. text\n","        self.target_column = target_column   # e.g. emotion\n","\n","        self.word2vec = word2vec\n","\n","        self.label2num = lambda label: 0 if label == 0 else 1\n","        self.mean = np.mean(word2vec.vectors, axis=0)\n","        self.std = np.std(word2vec.vectors, axis=0)\n","\n","    def __getitem__(self, item):\n","        # feature\n","        text = self.data[self.feature_column][item]\n","        tokens = self.get_tokens_(text)\n","        embeddings = self.get_embeddings_(tokens)\n","        # label\n","        label = self.label2num(self.data[self.target_column][item])\n","\n","        return {\"feature\": embeddings, \"target\": label}\n","\n","    def get_tokens_(self, text):\n","        # Получи все токены из текста и профильтруй их\n","        tokens = self.tokenizer.tokenize(text.lower())\n","        filtered_tokens = [t for t in tokens if all(c not in string.punctuation for c in t) and len(t) > 3]\n","        return filtered_tokens\n","\n","    def get_embeddings_(self, tokens):\n","        # Получи эмбеддинги слов и усредни их\n","        # `and len(t) > 3` is not needed (done during .get_tokens())\n","        embeddings = [(self.word2vec.get_vector(t) - self.mean) / self.std for t in tokens if t in self.word2vec]\n","        \n","        if len(embeddings) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            embeddings = np.array(embeddings)\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","\n","        return embeddings\n","\n","    def __len__(self):\n","        return self.data.shape[0]"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZJpttbXpZyz"},"source":["dev = TwitterDataset(data=dev_data,\n","                     feature_column=\"text\", target_column=\"emotion\",\n","                     word2vec=word2vec)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sr-aetH0_LH1"},"source":["Отлично, мы готовы с помощью эмбеддингов слов превращать твиты в векторы и обучать нейронную сеть.\n","\n","Превращать твиты в векторы, используя эмбеддинги слов, можно несколькими способами. А именно такими:"]},{"cell_type":"markdown","metadata":{"id":"4AhHrWa196Yc"},"source":["## Average embedding (2 балла)\n","---\n","Это самый простой вариант, как получить вектор предложения, используя векторные представления слов в предложении. А именно: вектор предложения есть средний вектор всех слов в предлоежнии (которые остались после токенизации и удаления коротких слов, конечно). "]},{"cell_type":"code","metadata":{"id":"ScdokSW-994t"},"source":["indexes = np.arange(len(dev))\n","np.random.shuffle(indexes)\n","example_indexes = indexes[::1000]\n","\n","# changed np.sum to np.mean\n","examples = {\"features\": [np.mean(dev[i][\"feature\"], axis=0) for i in example_indexes], \n","            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n","\n","\n","print(len(examples[\"features\"]), len(examples[\"features\"][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1yGQ_lOx_1NL"},"source":["Давайте сделаем визуализацию полученных векторов твитов тренировочного (dev) датасета. Так мы увидим, насколько хорошо твиты с разными target значениями отделяются друг от друга, т.е. насколько хорошо усреднение эмбеддингов слов предложения передает информацию о предложении."]},{"cell_type":"markdown","metadata":{"id":"LZwFksd_8uYO"},"source":["Для визуализации векторов надо получить их проекцию на плоскость. Сделаем это с помощью `PCA`. Если хотите, можете вместо PCA использовать TSNE: так у вас получится более точная проекция на плоскость (а значит, более информативная, т.е. отражающая реальное положение векторов твитов в пространстве). Но TSNE будет работать намного дольше."]},{"cell_type":"code","metadata":{"id":"zvCisN96WN5f"},"source":["type(examples['features'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFJ2okQ0NabH","executionInfo":{"status":"ok","timestamp":1615113368119,"user_tz":0,"elapsed":727,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["from sklearn.decomposition import PCA"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"aKFZRSHdtIac"},"source":["pca = PCA(n_components=2)\n","# Обучи PCA на эмбеддингах слов\n","examples[\"transformed_features\"] = pca.fit_transform(X=examples['features'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PyF4odNaWy82"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"szEOWdiNtIX8","executionInfo":{"status":"ok","timestamp":1615113372502,"user_tz":0,"elapsed":1245,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["import bokeh.models as bm, bokeh.plotting as pl\n","from bokeh.io import output_notebook\n","output_notebook()\n","\n","def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n","                 width=600, height=400, show=True, **kwargs):\n","    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n","    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n","\n","    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n","    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n","\n","    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n","    if show: pl.show(fig)\n","    return fig"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"7OONK8ldtIWe"},"source":["draw_vectors(\n","    examples[\"transformed_features\"][:, 0], \n","    examples[\"transformed_features\"][:, 1], \n","    color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fNF6LRQ9MPI"},"source":["Скорее всего, на визуализации нет четкого разделения твитов между классами. Это значит, что по полученным нами векторам твитов не так-то просто определить, к какому классу твит пренадлежит. Значит, обычный линейный классификатор не очень хорошо справится с задачей. Надо будет делать глубокую (хотя бы два слоя) нейронную сеть.\n","\n","Подготовим загрузчики данных.\n","Усреднее векторов будем делать в \"батчевалке\"(`collate_fn`). Она используется для того, чтобы собирать из данных `torch.Tensor` батчи, которые можно отправлять в модель.\n"]},{"cell_type":"code","metadata":{"id":"y1XapsADtITv","executionInfo":{"status":"aborted","timestamp":1615117619321,"user_tz":0,"elapsed":5241,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["from torch.utils.data import DataLoader\n","\n","\n","batch_size = 1024\n","num_workers = 4\n","\n","def average_emb(batch):\n","    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n","    targets = [b[\"target\"] for b in batch]\n","\n","    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YsB_WuBDKj2t"},"source":["train_size = math.ceil(len(dev) * 0.8)\n","train, valid = random_split(dev, [train_size, len(dev) - train_size])\n","\n","train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n","valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzUeLgxU_hUa"},"source":["for batch in train_loader:\n","    print(batch['features'].shape)\n","    print(batch['targets'].shape)\n","    print()\n","\n","    print(batch['features'][:20])\n","    print(batch['targets'][:20])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-zs0WEK-Vkt"},"source":["Определим функции для тренировки и теста модели:"]},{"cell_type":"code","metadata":{"id":"U--T2Gjw1r27","executionInfo":{"status":"ok","timestamp":1615117671497,"user_tz":0,"elapsed":370,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["from tqdm.notebook import tqdm\n","\n","def training(model, optimizer, criterion, train_loader, epoch, device=\"cpu\"):\n","    pbar = tqdm(train_loader, desc=f\"Epoch {e + 1}. Train Loss: {0}\")\n","    model.train()\n","    for batch in pbar:\n","        features = batch[\"features\"].to(device)\n","        targets = batch[\"targets\"].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(features)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        pbar.set_description(f\"Epoch {e + 1}. Train Loss: {loss:.4}\")\n","    \n","\n","def testing(model, criterion, test_loader, device=\"cpu\"):\n","    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n","    mean_loss = 0\n","    mean_acc = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in pbar:\n","            features = batch[\"features\"].to(device)\n","            targets = batch[\"targets\"].to(device)\n","\n","            # make predictions and calculate loss\n","            outputs = model(features)\n","            loss = criterion(outputs, targets)\n","\n","            # get class predictions and calculate the accuracy\n","            _, preds = torch.max(outputs, 1)\n","            acc = (preds == targets).to(torch.float).mean()            \n","\n","            mean_loss += loss.item()\n","            mean_acc += acc.item()\n","\n","            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n","\n","    pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n","\n","    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVg_XBBb-YBH"},"source":["Создадим модель, оптимизатор и целевую функцию. Вы можете сами выбрать количество слоев в нейронной сети, ваш любимый оптимизатор и целевую функцию.\n"]},{"cell_type":"code","metadata":{"id":"EBoZ4F3Fx1Hm"},"source":["import torch.nn as nn\n","from torch.optim import Adam\n","\n","\n","# Не забудь поиграться с параметрами ;)\n","vector_size = dev.word2vec.vector_size\n","num_classes = 2\n","lr = 1e-3\n","num_epochs = 5\n","\n","# TODO: define the model, loss, and optimiser\n","model = nn.Sequential(\n","    nn.Linear(vector_size, 200),\n","    nn.ReLU(),\n","\n","    nn.Linear(200, 100),\n","    nn.ReLU(),\n","\n","    nn.Linear(100, num_classes),\n",")\n","model = model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AitU8AR-zBj"},"source":["Наконец, обучим модель и протестируем её.\n","\n","После каждой эпохи будем проверять качество модели на валидационной части датасета. Если метрика стала лучше, будем сохранять модель. **Подумайте, какая метрика (точность или лосс) будет лучше работать в этой задаче?**\n","\n","У нас здесь стандартная задача классификации на 2 класса. Loss будет работать лучше потому что при уменьшении Loss-a для какого-то конкретного примера, мы будем приближаться к правильному ответу, а значит, будем увеличивать тосность. В отличие от точности, Loss дифференцируем и градиенты будут течь лучше."]},{"cell_type":"code","metadata":{"id":"gKhk71Pmx1F1"},"source":["# best_metric = np.inf\n","# for e in range(num_epochs):\n","#     training(model, optimizer, criterion, train_loader, e, device)\n","#     log = testing(model, criterion, valid_loader, device)\n","#     print(log)\n","#     if log[\"Test Loss\"] < best_metric:\n","#         torch.save(model.state_dict(), \"model.pt\")\n","#         best_metric = log[\"Test Loss\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"di4dGwD4x1Dt"},"source":["test_loader = DataLoader(\n","    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n","    batch_size=batch_size, \n","    num_workers=num_workers, \n","    shuffle=False,\n","    drop_last=False, \n","    collate_fn=average_emb)\n","\n","model.load_state_dict(torch.load(\"model_baseline.pt\", map_location=device))\n","\n","print(testing(model, criterion, test_loader, device=device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVciUaRMHfUG"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRvzpldHSAu0"},"source":["## Embeddings for unknown words (8 баллов)\n","\n","Пока что использовалась не вся информация из текста. Часть информации фильтровалось – если слова не было в словаре эмбеддингов, то мы просто превращали слово в нулевой вектор. Хочется использовать информацию по-максимуму. Поэтому рассмотрим другие способы обработки слов, которых нет в словаре. А именно:\n","\n","- Для каждого незнакомого слова будем запоминать его контекст(слова слева и справа от этого слова). Эмбеддингом нашего незнакомого слова будет сумма эмбеддингов всех слов из его контекста. (4 балла)\n","- Для каждого слова текста получим его эмбеддинг из Tfidf с помощью ```TfidfVectorizer``` из [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). Итоговым эмбеддингом для каждого слова будет сумма двух эмбеддингов: предобученного и Tfidf-ного. Для слов, которых нет в словаре предобученных эмбеддингов, результирующий эмбеддинг будет просто полученный из Tfidf. (4 балла)\n","\n","Реализуйте оба варианта **ниже**. Напишите, какой способ сработал лучше и ваши мысли, почему так получилось."]},{"cell_type":"code","metadata":{"id":"RxhEpKalU1UQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"47zAnu32XFIg"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"DX-6lVOtJkx_"},"source":["## Embedding of an unknown word = sum of embeddings of the context words"]},{"cell_type":"markdown","metadata":{"id":"DYd-ahBAy3KY"},"source":["### Эксперимент 1: Naive Context (1 left, 1 right)\n","\n","**Изменения** в `get_embeddings`\n","\n","1. Проходимся по всем `tokens`\n","    1. Если в `word2vec`, добавляем в `embeddings` как в baseline версии нормализуя его по self.mean, self.sdt\n","    2. Если не в `word2vec`, то заменяем на 'UNK' и добавлеяем нулевой embedding чтобы размер списка embeddings совпадал с размером списка tokens\n","\n","2. Проходимся еще раз по всем токенам слева направо\n","    1. Если `token=='UNK'`, заменяем его embedding на среднее между embeddings слева и справа\n","    2. Только справа для индекса 0, только слева для индекса len(tokens)-1\n","    3. Нормализуем с self.mean, self.sdt\n","\n","3. Повторям все как в оригинальной версии после list comprehension\n","\n","**Code**\n","\n","```python\n","class TwitterDatasetContextEmbeddings(TwitterDataset):\n","\n","    def get_embeddings_(self, tokens):\n","\n","        embeddings = []\n","        for i, t in enumerate(tokens):\n","\n","            # exactly the same as before in the list comprehension\n","            if t in self.word2vec:\n","                emb = (self.word2vec.get_vector(t) - self.mean) / self.std\n","                embeddings.append(emb)\n","            \n","            # new part: instead of just skipping a word, represent it through its context\n","            else:\n","                tokens[i] = '<UNK>'\n","                embeddings.append([0 for _ in range(self.word2vec.vector_size)])\n","\n","        # fill in missing embeddings, not ideal\n","        for i, t in enumerate(tokens):\n","\n","            if t == '<UNK>':\n","                if i == 0:\n","                    try:\n","                        emb = embeddings[i+1]\n","                    except IndexError:\n","                        # only one token and it's unknown\n","                        #print(\"len(tokens):\", len(tokens))\n","                        #print(\"Tokens:\", tokens)\n","                        emb = [0 for _ in range(self.word2vec.vector_size)]\n","                elif i == len(tokens)-1:\n","                    emb = embeddings[i-1]\n","                else:\n","                    emb = np.mean([embeddings[i-1], embeddings[i+1]], axis=0)\n","                \n","                # normalise and record\n","                emb = (emb - self.mean) / self.std\n","                embeddings[i] = emb\n","            else:\n","                pass\n","        \n","        # from here, everything is the same as before\n","        if len(embeddings) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            embeddings = np.array(embeddings)\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","\n","        return embeddings\n","```\n","\n","**PCA**\n","\n","Если смотреть на PCA, по PC2 variance уже намного меньше чем по PC1. Все сжалось по 2 компоненту. Скорее всего это произошло потому что до нормализации мы добавили много нулевых/очень маленьких по норме embeddings.\n","\n","**Проблемa:**\n","\n","Если у 'UNK' соседи тоже 'UNK', тогда мы заменяем плохо:\n","\n","Для `[token1, 'UNK', 'UNK', 'UNK', token2]`, мы получим следующие embeddings:\n","\n","\n","`[emb_t1, emb_t1/2, emb_t1/4, emb_t1/8 + emb_t2/2, emb_t2]`, \n","    \n","а потом еще и нормализуем чтобы быть в отдной размерности со словами из словаря `word2vec`\n","\n","**Результат:**\n","    \n","* Лучшая val   accuracy: 0.619\n","* Лучшая test  accuracy: 0.588\n","\n","Намного хуже, чем у baseline модели.\n","\n","**Вывод**\n","\n","Надо менять способ получения контекста. Попробуем взять первое ненулевое слово.\n","\n","----\n","\n"]},{"cell_type":"code","metadata":{"id":"DkFWY1raHVfi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZEd1zJqBj3D"},"source":["### Эксперимент 2: Existing Context  (1 left, 1 right)\n","\n","**Изменения** в `get_embeddings`\n","\n","1. Проходимся по всем `tokens`\n","    1. Если в `word2vec`, добавляем в `embeddings` как в baseline версии нормализуя его по self.mean, self.sdt\n","    2. Если не в `word2vec`, то заменяем на 'UNK' и добавлеяем нулевой embedding чтобы размер списка embeddings совпадал с размером списка tokens\n","\n","2. Проходимся еще раз по всем токенам слева направо\n","    1. Если `token=='UNK'`, заменяем его embedding на среднее между embeddings слов слева и справа, которые **не 'UNK'**\n","    2. Только справа для индекса 0, только слева для индекса len(tokens)-1\n","    3. Нормализуем с self.mean, self.sdt\n","\n","3. Повторям все как в оригинальной версии после list comprehension\n","\n","**Code**\n","\n","```python\n","class TwitterDatasetContextEmbeddings(TwitterDataset):\n","\n","\n","    def get_right_context_word_embedding_(self, tokens, embeddings, i):\n","        if i == len(tokens):\n","            return [0 for _ in range(self.word2vec.vector_size)]\n","        elif tokens[i] != '<UNK>':\n","            return embeddings[i]\n","        else:\n","            return self.get_right_context_word_embedding_(tokens, embeddings, i+1)\n","\n","    def get_left_context_word_embedding_(self, tokens, embeddings, i):\n","        if i == -1:\n","            return [0 for _ in range(self.word2vec.vector_size)]\n","        elif tokens[i] != '<UNK>':\n","            return embeddings[i]\n","        else:\n","            return self.get_left_context_word_embedding_(tokens, embeddings, i-1)\n","\n","    def get_embeddings_(self, tokens):\n","\n","        embeddings = []\n","        for i, t in enumerate(tokens):\n","\n","            # exactly the same as before in the list comprehension\n","            if t in self.word2vec:\n","                emb = (self.word2vec.get_vector(t) - self.mean) / self.std\n","                embeddings.append(emb)\n","            \n","            # new part: instead of just skipping a word, represent it through its context\n","            else:\n","                tokens[i] = '<UNK>'\n","                embeddings.append(None) # placeholder for indexes\n","\n","        # fill in missing embeddings, not ideal\n","        for i, t in enumerate(tokens):\n","\n","            if t == '<UNK>':\n","\n","                l_emb = self.get_left_context_word_embedding_(tokens, embeddings, i)\n","                r_emb = self.get_right_context_word_embedding_(tokens, embeddings, i)\n","\n","                emb = np.mean([l_emb, r_emb], axis=0)\n","                \n","                # normalise and record\n","                emb = (emb - self.mean) / self.std\n","                embeddings[i] = emb\n","            else:\n","                pass\n","        \n","        # from here, everything is the same as before\n","        if len(embeddings) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            embeddings = np.array(embeddings)\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","\n","        return embeddings\n","```\n","\n","**PCA**\n","\n","PCA стал опять похож на baseline случай.\n","\n","**Проблемa:**\n","\n","Если у 'UNK' соседи тоже 'UNK', тогда мы заменяем плохо:\n","\n","Для `[token1, 'UNK', 'UNK', 'UNK', token2]`, мы получим следующие embeddings:\n","\n","\n","`[emb_t1, (emb_t1+emb_t1)/2, (emb_t1+emb_t1)/2, (emb_t1+emb_t1)/2, emb_t2]`, \n","    \n","\n","1. Но для самого левого `'UNK'`, `token2` может вообще быть вне контекста.\n","2. Плюс, `token1` должен быть важнее, чем `token2`.\n","\n","\n","**Результат:**\n","    \n","* Лучшая val   accuracy: 0.743\n","* Лучшая test  accuracy: 0.747\n","\n","Чуть хуже, чем у baseline модели. Никакаго overfit-а на validation data не происходит, что хорошо.\n","\n","**Вывод**\n","\n","Можно попробовать улучшить способ получения контекста. Попробуем взять первое ненулевое слово в заданном окне.\n","\n","Q: Что делать если их там нет?\n","A: Можно просто пропучкать это слово, как делали в baseline версии"]},{"cell_type":"markdown","metadata":{"id":"DKri0QXWnpqx"},"source":[""]},{"cell_type":"code","metadata":{"id":"hqhsg0nMiLW9"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hxiUz3QiNa1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HufOoD58G7W_"},"source":["### Эксперимент 3: Existing Context  (1 left, 1 right inside a pre-specified window=3 of context)\n","\n","**Изменения** в `get_embeddings`\n","\n","1. Проходимся по всем `tokens`\n","    1. Если в `word2vec`, добавляем в `embeddings` как в baseline версии нормализуя его по self.mean, self.sdt\n","    2. Если не в `word2vec`, то заменяем на 'UNK' и добавлеяем нулевой embedding чтобы размер списка embeddings совпадал с размером списка tokens\n","\n","2. Проходимся еще раз по всем токенам слева направо\n","    1. Если `token=='UNK'`, заменяем его embedding на среднее между embeddings слов слева и справа внутри окна ширины 3, которые **не 'UNK'**\n","        * Только справа для индекса 0, только слева для индекса len(tokens)-1\n","        * Если получили ненулевой эмбеддинг\n","            * Нормализуем с self.mean, self.sdt\n","            * Добавляем индекс в **список с полезными индексами**\n","    2. Если `token!='UNK'`, то добавляем индекс в **список с полезными индексами**\n","\n","3. Повторям логику из оригинальной версии после list comprehension\n","\n","**Code**\n","\n","```python\n","class TwitterDatasetContextEmbeddings(TwitterDataset):\n","    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec,\n","                 emb_window):\n","        super().__init__(data, feature_column, target_column, word2vec)\n","        self.emb_window = emb_window\n","\n","    def get_right_context_word_embedding_(self, tokens, embeddings, i, r_bound):\n","        if i == min(len(tokens), r_bound+1):\n","            return np.zeros(self.word2vec.vector_size)\n","        elif tokens[i] != '<UNK>':\n","            return embeddings[i]\n","        else:\n","            return self.get_right_context_word_embedding_(tokens, embeddings, i+1, r_bound)\n","\n","    def get_left_context_word_embedding_(self, tokens, embeddings, i, l_bound):\n","        if i == max(-1, l_bound):\n","            return np.zeros(self.word2vec.vector_size)\n","        elif tokens[i] != '<UNK>':\n","            return embeddings[i]\n","        else:\n","            return self.get_left_context_word_embedding_(tokens, embeddings, i-1, l_bound)\n","\n","    def get_embeddings_(self, tokens):\n","\n","        indexes_to_keep = []\n","        embeddings = []\n","        for i, t in enumerate(tokens):\n","\n","            # exactly the same as before in the list comprehension\n","            if t in self.word2vec:\n","                emb = (self.word2vec.get_vector(t) - self.mean) / self.std\n","                embeddings.append(emb)\n","            \n","            # new part: instead of just skipping a word, represent it through its context\n","            else:\n","                tokens[i] = '<UNK>'\n","                embeddings.append(np.zeros(self.word2vec.vector_size)) # placeholder for indexes\n","\n","\n","        # fill in missing embeddings, not ideal, but even better\n","        for i, t in enumerate(tokens):\n","\n","            if t == '<UNK>':\n","\n","                l_emb = self.get_left_context_word_embedding_(tokens, embeddings, i, i-self.emb_window)\n","                r_emb = self.get_right_context_word_embedding_(tokens, embeddings, i, i+self.emb_window)\n","\n","                emb = np.mean([l_emb, r_emb], axis=0)\n","\n","                # record indexes of tokes to completely remove\n","                if all(emb < 1e-8):\n","                    pass\n","                else: \n","                    # normalise, record, and note that we'll keep\n","                    emb = (emb - self.mean) / self.std\n","                    embeddings[i] = emb\n","                    indexes_to_keep.append(i)\n","\n","            else:\n","                indexes_to_keep.append(i)\n","        \n","        # keep only relevant indormation if there is any\n","        if len(indexes_to_keep) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            indexes_to_keep = np.array(indexes_to_keep)\n","            embeddings = np.array(embeddings)\n","            embeddings = embeddings[indexes_to_keep]\n","        \n","            # reshape if necessary\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","        \n","\n","        return embeddings\n","```\n","\n","**PCA**\n","\n","PCA опять похож на baseline случай.\n","\n","**Проблемa:**\n","\n","Если у 'UNK' соседи тоже 'UNK', тогда мы заменяем плохо:\n","\n","Для `[token1, 'UNK', 'UNK', 'UNK', token2]`, мы получим следующие embeddings:\n","\n","\n","`[emb_t1, (emb_t1+emb_t1)/2, (emb_t1+emb_t1)/2, (emb_t1+emb_t1)/2, emb_t2]`, \n","    \n","Из прошлого эксперимента: для левого `'UNK'`, `token1` должен быть важнее, чем `token2`. Нужно добавить веса, а еще можно учитывать всех знакомых (не `'UNK'`) соседей внутри окна.\n","\n","\n","**Результат:**\n","    \n","* Лучшая val   accuracy: 0.744\n","* Лучшая test  accuracy: 0.750\n","\n","Опять чуть хуже, чем у baseline модели. Никакаго overfit-а на validation data не происходит, что хорошо.\n","\n","Результаты лучше, чем в эксперименте 2, но, на мой взгляд, в пределах погрешности.\n","\n","**Вывод**\n","\n","Можно попробовать еще улучшить способ получения контекста. Попробуем взять все ненулевые слова в заданном окне а еще и взвесить их. Сумма весов должна быть 1."]},{"cell_type":"code","metadata":{"id":"C2A6ImA97HdW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btpDYywO7IX0"},"source":["### Эксперимент 4: Existing Context  (all left, all right inside a pre-specified window=3 of context weighted inversly proportional to distance from the central word)\n","\n","**Изменения** в `get_embeddings`\n","\n","1. Проходимся по всем `tokens`\n","    1. Если в `word2vec`, добавляем в `embeddings` как в baseline версии нормализуя его по self.mean, self.sdt\n","    2. Если не в `word2vec`, то заменяем на 'UNK' и добавлеяем нулевой embedding чтобы размер списка embeddings совпадал с размером списка tokens\n","\n","2. Проходимся еще раз по всем токенам слева направо\n","    1. Если `token=='UNK'`, заменяем его embedding на среднее между dctvb embeddings слов слева и справа внутри окна ширины 3, которые **не 'UNK'** взвешивая обратно-пропорционально расстоянию от центрального слова.\n","        * Только справа для индекса 0, только слева для индекса len(tokens)-1\n","        * Если получили ненулевой эмбеддинг\n","            * Нормализуем с self.mean, self.sdt\n","            * Добавляем индекс в **список с полезными индексами**\n","    2. Если `token!='UNK'`, то добавляем индекс в **список с полезными индексами**\n","\n","3. Повторям логику из оригинальной версии после list comprehension\n","\n","**Code**\n","\n","```python\n","\n","class TwitterDatasetContextEmbeddings(TwitterDataset):\n","    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec,\n","                 emb_window):\n","        super().__init__(data, feature_column, target_column, word2vec)\n","        self.emb_window = emb_window\n","    \n","\n","    def get_embeddings_(self, tokens):\n","\n","        indexes_to_keep = []\n","        embeddings = np.zeros((len(tokens), self.word2vec.vector_size))\n","\n","        for i, t in enumerate(tokens):\n","\n","            # exactly the same as before in the list comprehension\n","            if t in self.word2vec:\n","                emb = (self.word2vec.get_vector(t) - self.mean) / self.std\n","                embeddings[i] = emb\n","            \n","            # new part: instead of just skipping a word, represent it through its context\n","            else:\n","                tokens[i] = '<UNK>'\n","\n","        # fill in missing embeddings, close to ideal, but messy implementation\n","        for i, t in enumerate(tokens):\n","\n","            if t == '<UNK>':\n","\n","                l_bound = max(0, i-self.emb_window)\n","                r_bound = min(len(tokens)-1, i+self.emb_window)\n","                context_inds = np.array([ind for ind in range(l_bound, r_bound+1) if ind != i])\n","\n","                if len(context_inds) != 0:\n","                    context = embeddings[context_inds].copy()\n","\n","                    # do not include unknown words as context\n","                    for ci in range(len(context)):\n","                        if tokens[context_inds[ci]] == '<UNK>':\n","                            context[ci] = np.zeros(self.word2vec.vector_size)\n","\n","                    # left weights\n","                    left_weights = []\n","                    w = 1\n","                    for _ in range(l_bound, i):\n","                        left_weights.append(w)\n","                        w +=1\n","                    # right weights\n","                    right_weights = []\n","                    w = 1\n","                    for _ in range(i+1, r_bound+1):\n","                        right_weights.append(w)\n","                        w +=1\n","                    right_weights.reverse()\n","                    # all weights\n","                    weights = np.array(left_weights + right_weights)\n","                    weights = weights / weights.sum()\n","                    weights = weights.reshape(-1, 1)\n","                    \n","                    # check\n","                    assert weights.shape[0] == context.shape[0]\n","\n","                    emb = np.mean(weights * context, axis=0)\n","                else:\n","                    emb = np.zeros(self.word2vec.vector_size)\n","\n","                # record indexes of tokes with meaningful embeddings\n","                if all(emb < 1e-8):\n","                    pass\n","                else: \n","                    # normalise, record, and note that we'll keep\n","                    emb = (emb - self.mean) / self.std\n","                    embeddings[i] = emb\n","                    indexes_to_keep.append(i)\n","\n","            else:\n","                indexes_to_keep.append(i)\n","        \n","        # keep only relevant indormation if there is any\n","        if len(indexes_to_keep) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            indexes_to_keep = np.array(indexes_to_keep)\n","            embeddings = np.array(embeddings)\n","            embeddings = embeddings[indexes_to_keep]\n","        \n","            # reshape if necessary\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","        \n","        return embeddings\n","\n","```\n","\n","**PCA**\n","\n","PCA опять похож на baseline случай.\n","\n","\n","**Результат:**\n","    \n","* Лучшая val   accuracy: 0.755\n","* Лучшая test  accuracy: 0.729\n","\n","Опять чуть хуже, чем у baseline модели. Пошел overfit-а на validation data. Пора заканчивать с этим. Baseline так и не получилось побить.\n","\n","**Вывод**\n","\n","Пора пробовать tf-idf метод :)"]},{"cell_type":"code","metadata":{"id":"EG-Kr0_n7Ix4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HwrDE4ta9rs9"},"source":["### Final Code"]},{"cell_type":"code","metadata":{"id":"wu4eofymhS_0","executionInfo":{"status":"ok","timestamp":1615113404040,"user_tz":0,"elapsed":386,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["class TwitterDatasetContextEmbeddings(TwitterDataset):\n","    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec,\n","                 emb_window):\n","        super().__init__(data, feature_column, target_column, word2vec)\n","        self.emb_window = emb_window\n","    \n","\n","    def get_embeddings_(self, tokens):\n","\n","        indexes_to_keep = []\n","        embeddings = np.zeros((len(tokens), self.word2vec.vector_size))\n","\n","        for i, t in enumerate(tokens):\n","\n","            # exactly the same as before in the list comprehension\n","            if t in self.word2vec:\n","                emb = (self.word2vec.get_vector(t) - self.mean) / self.std\n","                embeddings[i] = emb\n","            \n","            # new part: instead of just skipping a word, represent it through its context\n","            else:\n","                tokens[i] = '<UNK>'\n","\n","        # fill in missing embeddings, close to ideal, but messy implementation\n","        for i, t in enumerate(tokens):\n","\n","            if t == '<UNK>':\n","\n","                l_bound = max(0, i-self.emb_window)\n","                r_bound = min(len(tokens)-1, i+self.emb_window)\n","                context_inds = np.array([ind for ind in range(l_bound, r_bound+1) if ind != i])\n","\n","                if len(context_inds) != 0:\n","                    context = embeddings[context_inds].copy()\n","\n","                    # do not include unknown words as context\n","                    for ci in range(len(context)):\n","                        if tokens[context_inds[ci]] == '<UNK>':\n","                            context[ci] = np.zeros(self.word2vec.vector_size)\n","\n","                    # left weights\n","                    left_weights = []\n","                    w = 1\n","                    for _ in range(l_bound, i):\n","                        left_weights.append(w)\n","                        w +=1\n","                    # right weights\n","                    right_weights = []\n","                    w = 1\n","                    for _ in range(i+1, r_bound+1):\n","                        right_weights.append(w)\n","                        w +=1\n","                    right_weights.reverse()\n","                    # all weights\n","                    weights = np.array(left_weights + right_weights)\n","                    weights = weights / weights.sum()\n","                    weights = weights.reshape(-1, 1)\n","                    \n","                    # check\n","                    assert weights.shape[0] == context.shape[0]\n","\n","                    emb = np.mean(weights * context, axis=0)\n","                else:\n","                    emb = np.zeros(self.word2vec.vector_size)\n","\n","                # record indexes of tokes with meaningful embeddings\n","                if all(emb < 1e-8):\n","                    pass\n","                else: \n","                    # normalise, record, and note that we'll keep\n","                    emb = (emb - self.mean) / self.std\n","                    embeddings[i] = emb\n","                    indexes_to_keep.append(i)\n","\n","            else:\n","                indexes_to_keep.append(i)\n","        \n","        # keep only relevant indormation if there is any\n","        if len(indexes_to_keep) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            indexes_to_keep = np.array(indexes_to_keep)\n","            embeddings = np.array(embeddings)\n","            embeddings = embeddings[indexes_to_keep]\n","        \n","            # reshape if necessary\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","        \n","        return embeddings"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"tof2EWXCNnZ7"},"source":["dev = TwitterDatasetContextEmbeddings(\n","    data=dev_data,\n","    feature_column=\"text\",\n","    target_column=\"emotion\",\n","    word2vec=word2vec,\n","    emb_window=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjJtSJACWLO0"},"source":["indexes = np.arange(len(dev))\n","np.random.shuffle(indexes)\n","example_indexes = indexes[::1000]\n","\n","# changed np.sum to np.mean\n","examples = {\"features\": [np.mean(dev[i][\"feature\"], axis=0) for i in example_indexes], \n","            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n","\n","\n","print(len(examples[\"features\"]), len(examples[\"features\"][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"man0CSkYX_mg"},"source":["pca = PCA(n_components=2)\n","# Обучи PCA на эмбеддингах слов\n","examples[\"transformed_features\"] = pca.fit_transform(X=examples['features'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7gZYXGqaSrT"},"source":["draw_vectors(\n","    examples[\"transformed_features\"][:, 0], \n","    examples[\"transformed_features\"][:, 1], \n","    color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjG6eEvtckr6"},"source":["from torch.utils.data import DataLoader\n","\n","\n","batch_size = 1024\n","num_workers = 4\n","\n","def average_emb(batch):\n","    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n","    targets = [b[\"target\"] for b in batch]\n","\n","    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n","\n","\n","train_size = math.ceil(len(dev) * 0.8)\n","\n","train, valid = random_split(dev, [train_size, len(dev) - train_size])\n","\n","train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n","valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoUs1gIccQ0K"},"source":["import torch.nn as nn\n","from torch.optim import Adam\n","\n","\n","# TODO: try different parameters\n","vector_size = dev.word2vec.vector_size\n","num_classes = 2\n","lr = 1e-3\n","num_epochs = 5\n","\n","# TODO: define the model, loss, and optimiser\n","model = nn.Sequential(\n","    nn.Linear(vector_size, 200),\n","    nn.ReLU(),\n","\n","    nn.Linear(200, 100),\n","    nn.ReLU(),\n","\n","    nn.Linear(100, num_classes),\n",")\n","model = model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4wxdBK0aYKw"},"source":["best_metric = np.inf\n","for e in range(num_epochs):\n","    training(model, optimizer, criterion, train_loader, e, device)\n","    log = testing(model, criterion, valid_loader, device)\n","    print(log)\n","    if log[\"Test Loss\"] < best_metric:\n","        torch.save(model.state_dict(), \"model_context_11_window3_drop_zero.pt\")\n","        best_metric = log[\"Test Loss\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJsvPVBscWhZ"},"source":["test_loader = DataLoader(\n","    TwitterDatasetContextEmbeddings(test_data, \"text\", \"emotion\", word2vec, emb_window=3), \n","    batch_size=batch_size, \n","    num_workers=num_workers, \n","    shuffle=False,\n","    drop_last=False, \n","    collate_fn=average_emb)\n","\n","model.load_state_dict(torch.load(\"model_context_11_window3_drop_zero.pt\", map_location=device))\n","\n","print(testing(model, criterion, test_loader, device=device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5UrDdmNCfnhB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MljF-BSN9zTz"},"source":["## TF-IDF embeddings"]},{"cell_type":"code","metadata":{"id":"Jvv5AYoi5Rvc","executionInfo":{"status":"ok","timestamp":1615117689428,"user_tz":0,"elapsed":345,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.utils.extmath import randomized_svd"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdJuUKAf9C6R","executionInfo":{"status":"ok","timestamp":1615117690441,"user_tz":0,"elapsed":332,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["def dummy_fun(doc):\n","    return doc\n","\n","tfidf = TfidfVectorizer(\n","    analyzer='word',\n","    tokenizer=dummy_fun,\n","    preprocessor=dummy_fun,\n","    token_pattern=None)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"uD4C6-MB1EgP","executionInfo":{"status":"ok","timestamp":1615117691978,"user_tz":0,"elapsed":321,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["def my_get_tokens_(text, my_tokenizer=nltk.WordPunctTokenizer()):\n","    # Получи все токены из текста и профильтруй их\n","    tokens = my_tokenizer.tokenize(text.lower())\n","    filtered_tokens = [t for t in tokens if all(c not in string.punctuation for c in t) and len(t) > 3]\n","    return filtered_tokens"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YIEDpqsL2oc3"},"source":["### Get TF-IDF embedding for 1000 examples"]},{"cell_type":"code","metadata":{"id":"vir4HBeB1Eco"},"source":["examples = data[\"text\"].sample(1000)\n","print(\"\\n\\n\".join(examples[::100]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"seA2zIvE2u8i"},"source":["examples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ez8kHmUmLD4I"},"source":["docs = [my_get_tokens_(text) for text in examples]\n","docs[::100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nC-r6741EjS"},"source":["tfidf_matrix = tfidf.fit_transform(docs)\n","print(tfidf.vocabulary_)\n","\n","tfidf_matrix.todense().shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4upEFpNpJZO6"},"source":["tfidf_matrix.todense()[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_dmai5hNbHV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3RXZsAZ9NcIi"},"source":["### Get a 300-component SVD.\n","\n","Use:\n","\n","$$X = V \\Sigma U^T$$\n","\n","instead of the usual form\n","\n","$$X = U \\Sigma V^T$$\n","\n","**Why?**\n","\n","U is in our convention a matrix with word-vectors, while V - matrix with document/context vectors.\n"]},{"cell_type":"code","metadata":{"id":"sxrRMcA-JZuZ"},"source":["V, Sigma, UT = randomized_svd(tfidf_matrix, \n","                              n_components=300,\n","                              n_iter=5,\n","                              random_state=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8lzKZrifJZwY"},"source":["V.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKWcmdW8NNrV"},"source":["Sigma.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YHl9-iviNPS1"},"source":["UT.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9iuoc6UQPDmZ"},"source":["UT[:, 0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guYTqM1sQJlK"},"source":["### Get TF-IDF features for vocabulary in the example documents"]},{"cell_type":"code","metadata":{"id":"GjNDNO9bO3FU"},"source":["exampleword2tfidf = {}\n","for v in tfidf.vocabulary_:\n","    exampleword2tfidf[v] = UT[:, tfidf.vocabulary_[v]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8mqnZ0kdP-2Y"},"source":["exampleword2tfidf['make'].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZD3N8zAPVue"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6qcPOGRJqfN"},"source":["### Get TF-IDF embeddings for dev and test sets"]},{"cell_type":"code","metadata":{"id":"j8O8w-Jo0tnU","executionInfo":{"status":"ok","timestamp":1615117698674,"user_tz":0,"elapsed":1741,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["indexes = np.arange(data.shape[0])\n","np.random.shuffle(indexes)\n","dev_size = math.ceil(data.shape[0] * 0.8)\n","\n","dev_indexes = indexes[:dev_size]\n","test_indexes = indexes[dev_size:]\n","\n","dev_data = data.iloc[dev_indexes]\n","test_data = data.iloc[test_indexes]\n","\n","dev_data.reset_index(drop=True, inplace=True)\n","test_data.reset_index(drop=True, inplace=True)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NuxmWLHW5pM"},"source":["#### dev set\n","\n","Uncomment when doing first time. Takes time and RAM, so was precomputed separately and saved."]},{"cell_type":"code","metadata":{"id":"jJZ2X_J-0uGV","executionInfo":{"status":"ok","timestamp":1615117700418,"user_tz":0,"elapsed":434,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["# dev_examples = dev_data['text']\n","# dev_docs = [my_get_tokens_(text) for text in dev_examples]"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"FP3kvfb4TJbD","executionInfo":{"status":"ok","timestamp":1615117700418,"user_tz":0,"elapsed":284,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["# # X = V Sigma U^T\n","# UT = randomized_svd(tfidf.fit_transform(dev_docs),\n","#                     n_components=200,\n","#                     )[2]"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"m_yB3ezCTW-f"},"source":["# dev_word2tfidf = {}\n","# for v in tfidf.vocabulary_:\n","#     dev_word2tfidf[v] = UT[:, tfidf.vocabulary_[v]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_a89zBLBWCEy"},"source":["# import pickle\n","\n","# f = open(\"dev_word2tfidf.pkl\",\"wb\")\n","# pickle.dump(dev_word2tfidf,f)\n","# f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgIhrZviXCRX"},"source":["# len(dev_word2tfidf['make'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"22GTJTZ1W_lb"},"source":["#### test set\n","\n","Uncomment when doing first time. Takes time and RAM, so was precomputed separately and saved.\n","\n"]},{"cell_type":"code","metadata":{"id":"j72Yk-GpXSJ9"},"source":["# test_examples = test_data['text']\n","# test_docs = [my_get_tokens_(text) for text in test_examples]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRL_Z9dCTb9y"},"source":["# # X = V Sigma U^T, so we only need the UT component with index 2\n","# #\n","# # we do not fit a new one, but use the fitted on the dev set\n","# UT = randomized_svd(tfidf.transform(test_docs),\n","#                     n_components=200,\n","#                     )[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKs9vhpkXjXX"},"source":["# test_word2tfidf = {}\n","# for v in tfidf.vocabulary_:\n","#     test_word2tfidf[v] = UT[:, tfidf.vocabulary_[v]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKA9mZCPXwuG"},"source":["# import pickle\n","\n","# f = open(\"test_word2tfidf.pkl\",\"wb\")\n","# pickle.dump(test_word2tfidf,f)\n","# f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LpWx1NnnX56a"},"source":["# len(test_word2tfidf['make'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUoBxe97X9dL"},"source":["# # check: they should have different representation because the context and documents in test\n","# # are different\n","# any(test_word2tfidf['make'] == dev_word2tfidf['make'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xavhgKGJX9nB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rc2oUlewX9pJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dymqVts0X9rO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lhaU5ZgKBBo"},"source":["### TwitterDataset class for reference\n","\n","```python\n","\n","class TwitterDataset(Dataset):\n","    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n","        self.tokenizer = nltk.WordPunctTokenizer()\n","        \n","        self.data = data\n","\n","        self.feature_column = feature_column # e.g. text\n","        self.target_column = target_column   # e.g. emotion\n","\n","        self.word2vec = word2vec\n","\n","        self.label2num = lambda label: 0 if label == 0 else 1\n","        self.mean = np.mean(word2vec.vectors, axis=0)\n","        self.std = np.std(word2vec.vectors, axis=0)\n","\n","    def __getitem__(self, item):\n","        # feature\n","        text = self.data[self.feature_column][item]\n","        tokens = self.get_tokens_(text)\n","        embeddings = self.get_embeddings_(tokens)\n","        # label\n","        label = self.label2num(self.data[self.target_column][item])\n","\n","        return {\"feature\": embeddings, \"target\": label}\n","\n","    def get_tokens_(self, text):\n","        # Получи все токены из текста и профильтруй их\n","        tokens = self.tokenizer.tokenize(text.lower())\n","        filtered_tokens = [t for t in tokens if all(c not in string.punctuation for c in t) and len(t) > 3]\n","        return filtered_tokens\n","\n","    def get_embeddings_(self, tokens):\n","        # Получи эмбеддинги слов и усредни их\n","        # `and len(t) > 3` is not needed (done during .get_tokens())\n","        embeddings = [(self.word2vec.get_vector(t) - self.mean) / self.std for t in tokens if t in self.word2vec]\n","        \n","        if len(embeddings) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            embeddings = np.array(embeddings)\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","\n","        return embeddings\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"ZFdHicNTKJnc"},"source":["### TwitterDatasetTFIDFEmbeddings class"]},{"cell_type":"code","metadata":{"id":"2gstWSes3dTr","executionInfo":{"status":"ok","timestamp":1615117706159,"user_tz":0,"elapsed":341,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["class TwitterDatasetTFIDFEmbeddings(TwitterDataset):\n","    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec,\n","                 word2tfidf):\n","        super().__init__(data, feature_column, target_column, word2vec)\n","        self.word2tfidf = word2tfidf\n","\n","\n","    def get_embeddings_(self, tokens):\n","        # Получи эмбеддинги слов и усредни их\n","\n","        embeddings_pretrained = np.zeros((len(tokens), self.word2vec.vector_size))\n","        embeddings_tfidf = np.zeros((len(tokens), 200)) # 300 did not fit into RAM\n","        \n","        for i, t in enumerate(tokens):\n","            \n","            if t in self.word2tfidf:\n","                embeddings_tfidf[i] = self.word2tfidf.get(t)\n","            if t in self.word2vec:\n","                embeddings_pretrained[i] = (self.word2vec.get_vector(t) - self.mean) / self.std\n","\n","        embeddings = np.hstack([embeddings_pretrained, embeddings_tfidf])\n","\n","        assert embeddings.shape[1] == 500\n","\n","        if len(embeddings) == 0:\n","            embeddings = np.zeros((1, 500))\n","        else:\n","            embeddings = np.array(embeddings)\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","\n","        return embeddings"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R4EE3ooJbNSX"},"source":["### Final training"]},{"cell_type":"code","metadata":{"id":"8eG82PDfbT8D","executionInfo":{"status":"ok","timestamp":1615116075163,"user_tz":0,"elapsed":415,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["# import pickle\n","\n","# with open('dev_word2tfidf.pkl', 'rb') as f:\n","#     word2tfidf = pickle.load(f)"],"execution_count":104,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7J7zo13yVb-","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"error","timestamp":1615117712471,"user_tz":0,"elapsed":392,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"63bf404e-8338-44bb-cb90-a8bf36d35c4f"},"source":["dev = TwitterDatasetTFIDFEmbeddings(\n","    data=dev_data,\n","    feature_column=\"text\", target_column=\"emotion\",\n","    word2vec=word2vec,\n","    word2tfidf=word2tfidf)"],"execution_count":22,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-61a567fdbffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfeature_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"emotion\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword2vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     word2tfidf=word2tfidf)\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'word2tfidf' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"d5fNer1mZLNI","executionInfo":{"status":"error","timestamp":1615117712615,"user_tz":0,"elapsed":399,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"847189e7-b5b3-4b57-b9db-34d0b33c7fea"},"source":["len(dev)"],"execution_count":23,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-1abd7359e559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dev' is not defined"]}]},{"cell_type":"code","metadata":{"id":"fQ079s-taEMc","executionInfo":{"status":"aborted","timestamp":1615117619324,"user_tz":0,"elapsed":5193,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["indexes = np.arange(len(dev))\n","np.random.shuffle(indexes)\n","example_indexes = indexes[::1000]\n","\n","# changed np.sum to np.mean\n","examples = {\"features\": [np.mean(dev[i][\"feature\"], axis=0) for i in example_indexes], \n","            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n","\n","\n","print(len(examples[\"features\"]), len(examples[\"features\"][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6FKJcaNjYBQv","executionInfo":{"status":"aborted","timestamp":1615117619324,"user_tz":0,"elapsed":5189,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cX0TzSeAaEMi","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1615117726845,"user_tz":0,"elapsed":476,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"914ef87c-d7f1-4517-bb5c-1ad60c2e6500"},"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 1024\n","num_workers = 4\n","\n","def average_emb(batch):\n","    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n","    targets = [b[\"target\"] for b in batch]\n","\n","    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n","\n","\n","train_size = math.ceil(len(dev) * 0.8)\n","\n","train, valid = random_split(dev, [train_size, len(dev) - train_size])\n","\n","train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n","valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"],"execution_count":24,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-f85a3b37b32e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dev' is not defined"]}]},{"cell_type":"code","metadata":{"id":"9ZBhN6MuaEMi","executionInfo":{"status":"ok","timestamp":1615117740277,"user_tz":0,"elapsed":10102,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}}},"source":["import torch.nn as nn\n","from torch.optim import Adam\n","\n","\n","# TODO: try different parameters\n","\n","# 300 from word2vec\n","# 200 from tfidf\n","vector_size = 500\n","\n","num_classes = 2\n","lr = 1e-3\n","num_epochs = 5\n","\n","# TODO: define the model, loss, and optimiser\n","model = nn.Sequential(\n","    nn.Linear(vector_size, 200),\n","    nn.ReLU(),\n","\n","    nn.Linear(200, 100),\n","    nn.ReLU(),\n","\n","    nn.Linear(100, num_classes),\n",")\n","model = model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymrcNGHTaEMj","colab":{"base_uri":"https://localhost:8080/","height":595,"referenced_widgets":["8e295c0b9e3844879c528a8c6359e4b6","b93c5306a4964fa8b165fc2e514db46f","dacbef164ecc41b1b1e0a5cb56f7af7d","7d84d719d2f940e086ec78e5249fdf00","1a180c82af9b4ded8da072dd97cd53bf","07f707f851b54e77afa974c9a5a20105","17ffae7a77c94c26b61282ae1851051c","592c3bbacf6b42328800b872bfc7f549","a46023a1450d4a59b56908bcb1fce7e8","6e5fe77f909b4438986cd2b1915558ec","2bbf5deeadf643fe9a472b8933fb5239","5acd8c25a09f4700bef9c47a0ee27322","562c0451215f4cc3975de65e86ffc1fc","621bb5ec512d439c96e283747b1309bf","1d7354207c0a4d139c874caf309ec188","33e4100708724aae8fe71de2c9a358cb","ba175355a7f9484fb491cc363e6220ac","93d9109354344fb1921f8f3614251107","39057135209f4d92b2762235ea19b6a6","c7dba86c70e44fd2a8db2bda9ba92d21","c86f22e37d844852b7b7c75ef10ed377","b0be6e5e828d422c981372da7094082e","4a8ac5c2de864580b264a99926a9c171","db004ee4ac1d495cb48e55abffffdfdf","43a57e094a4b4533be9c624ac2e133b4","a37e1f570c1d49d5a834775c3bbc3643","062fd71a299e4a02b947c438d57f535b","f12c3f51631e4c23950682ec2aea5813","729e0169b51446b685b577d1ad728291","84068874d3ea448184fa230ed9ae3c35","156ac608c2b44d789f219fa46b6a4b19","c854291b49ce45c19b362cac44d6e704","aadfe725f8f34222a60dc67a2a9395ea","4b7989ca9482430296813ce1132f8f32","b4ff246a72b74576bff9745210df03bf","7bb408a7361042698d4778e127daa7bd","98a1f7ccc8de45a492947a90e8422c7d","5eafab15d51242eda9d8cb4fc49a649a","713517703bc74312939d5e84ebed8aac","152cb86d1cce4435ba05bb5026be5f33","d2b896ff9b514519bf240a2bff55dd3e","da1215fd93774f6b9ba6d25edfb4b1b9","648e8998d7754f1494580cd1d01bf9f0","7d8a11b8f2b84a05ab066ab50e9ff991","c7d5db1b5dbd47329852406c1282fe66","45dce49739b848ef8c0e3fd62074b012","86b44ccc9d384f83b1d563211f115608","3283713c9ef34a50961d40442f5f5e92","c239c5842362430f954119b567d96814","257c9991bd3546c4ae429500f738bea0","a2cd9ef6780a4cb98635a060aaa878f5","e9aa6eeea4c94242959743730460314f","bd2074d768e54428b860fe64f50d055c","79ac6e0f7b68469f9180ec81ce34765c","3693940645014511bccf75c419a9acb5","b04dbc1ceb094be18192cfbe47650a8d","8f1816759bb846299f41c079956f5e41","3409675434a1415a8a400ca9c8ca8ba8","3a6d4838428a448e834a4b9088674848","f098d977d54e405a8def86853d5b8a68","416e33fa5c83486598cbeef41c27e303","dcc42c0117f240249fa40303dc1b00ef","19a629f2cc9b463eac0e66be709b705f","bc58db755ef34d2c93b8c2404e8a1e29","16c7fe19129f47b28fb33221ae686625","d7f78e3f5b074afc83f2c5b910b8fb39","774a187f9d9c4fb18e8503eba113d996","68de190b024d4bed9ff8ec80a18e30ff","39428f9bbb5d4f549d63f620ca28ef5d","7067b8b7397848b3bb0c004fcec49e12","f0220a7733ea48d980e1e45d35725abc","7fdb53ba64734d8a97c0bcd3fee0471b","e39c492cf11b4f1abbfec062183658e6","bf71ebeb18be4b1a98800d0489a35c66","ece70eca441c4a08bbf0fdd1eabdf078","66cdf539a9e74648a7553c929ff7d986","4754e9d7dadf4ac8a1084e5ef35b13b5","ca19976432c74208b6494779269d4711","52ce62a7cca7460cbd235f2d87b03e2e","5306f7aee9854b5e952f8337241a7e18"]},"executionInfo":{"status":"ok","timestamp":1615117172739,"user_tz":0,"elapsed":1074182,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"cd4a8998-0329-43b0-9d4b-c3f0332f61de"},"source":["best_metric = np.inf\n","for e in range(num_epochs):\n","    training(model, optimizer, criterion, train_loader, e, device)\n","    log = testing(model, criterion, valid_loader, device)\n","    print(log)\n","    if log[\"Test Loss\"] < best_metric:\n","        torch.save(model.state_dict(), \"model_tfidf.pt\")\n","        best_metric = log[\"Test Loss\"]"],"execution_count":111,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e295c0b9e3844879c528a8c6359e4b6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 1. Train Loss: 0', max=1000.0, style=ProgressStyle(…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a46023a1450d4a59b56908bcb1fce7e8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Test Loss: 0, Test Acc: 0', max=250.0, style=ProgressStyl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","{'Test Loss': 0.49779582488536833, 'Test Acc': 0.75541015625}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba175355a7f9484fb491cc363e6220ac","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 2. Train Loss: 0', max=1000.0, style=ProgressStyle(…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43a57e094a4b4533be9c624ac2e133b4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Test Loss: 0, Test Acc: 0', max=250.0, style=ProgressStyl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","{'Test Loss': 0.49519594275951384, 'Test Acc': 0.757203125}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aadfe725f8f34222a60dc67a2a9395ea","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 3. Train Loss: 0', max=1000.0, style=ProgressStyle(…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2b896ff9b514519bf240a2bff55dd3e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Test Loss: 0, Test Acc: 0', max=250.0, style=ProgressStyl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","{'Test Loss': 0.4908740626573563, 'Test Acc': 0.760125}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c239c5842362430f954119b567d96814","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 4. Train Loss: 0', max=1000.0, style=ProgressStyle(…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f1816759bb846299f41c079956f5e41","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Test Loss: 0, Test Acc: 0', max=250.0, style=ProgressStyl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","{'Test Loss': 0.4914547641277313, 'Test Acc': 0.75971875}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"16c7fe19129f47b28fb33221ae686625","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 5. Train Loss: 0', max=1000.0, style=ProgressStyle(…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e39c492cf11b4f1abbfec062183658e6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Test Loss: 0, Test Acc: 0', max=250.0, style=ProgressStyl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","{'Test Loss': 0.4869244567155838, 'Test Acc': 0.762765625}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SKoyRvaOaEMj","colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["0a9ebc15fed045fcb805b6d25beccbd5","bcec1c1e246c445eb85a782523963b1c","13c57c146d9b4a11a120d5369e5c0c15","425419dd455c48019902361d482461d0","4b0f4ce7dd1345359740e1f4a9870080","c7166d2e8e9446828d92b871ef324f11","96570a115c3d46b681eed87c7e911b0c","47c3c4d95ee549adb62d1c700c5311b2"]},"executionInfo":{"status":"ok","timestamp":1615117835256,"user_tz":0,"elapsed":83921,"user":{"displayName":"George Batchkala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDONxhSnrmdSTlEhvc2cJI7El4zBMhqZjcA1cl1A=s64","userId":"05060257197849650568"}},"outputId":"f26e36db-0ae4-4226-cf07-6843db07b82f"},"source":["import pickle\n","\n","# rewrite word2tfidf to save RAM\n","with open('test_word2tfidf.pkl', 'rb') as f:\n","    word2tfidf = pickle.load(f)\n","\n","# create test loader\n","# note, we create the dataset inside\n","test_loader = DataLoader(\n","    TwitterDatasetTFIDFEmbeddings(test_data, \"text\", \"emotion\", word2vec,\n","                                  word2tfidf=word2tfidf), \n","    batch_size=batch_size, \n","    num_workers=num_workers, \n","    shuffle=False,\n","    drop_last=False, \n","    collate_fn=average_emb)\n","\n","model.load_state_dict(torch.load(\"model_tfidf.pt\", map_location=device))\n","\n","print(testing(model, criterion, test_loader, device=device))"],"execution_count":27,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a9ebc15fed045fcb805b6d25beccbd5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Test Loss: 0, Test Acc: 0', max=313.0, style=ProgressStyl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","{'Test Loss': 0.5195554243489957, 'Test Acc': 0.7385745307507987}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tIUG0yGHbGKj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SBSQRvWwhnK_"},"source":["### Итоги с TFIDF\n","\n","Лучшие:\n","\n","* Valid Accuracy: 0.763\n","* Test Accuracy : 0.739\n","\n","Validation accuracy лучше, чем когда либо, а вот Test accuracy упала. Произошёл overfit на dev set."]},{"cell_type":"code","metadata":{"id":"33sBu1_uhwuR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vqzTuMEkhzB-"},"source":["## Общий Вывод:\n","\n","Тяжело тренить, когда RAM не хватает :(\n","\n","Поэтому в ответах на печать будут только последние результаты с TFIDF.\n","\n","Делать фичи слова из контекста (первое улучшение бещ TF-IDF) вообще мне кажется не очень хорошей идеей потому что мы пытаемся взять линейную комбинацию фичей, которые у нас и так есть. ничего хорошего из этого не вышло.\n","\n","TFIDF поинтереснее, но тоже не дало улучшения. Можно бы еще с ним поковыряться, но пора сдавать, смотреть лекции по RNN, и начинать по ним домашку... Много всего :) \n"]},{"cell_type":"code","metadata":{"id":"XiGOqc_5jQNJ"},"source":[""],"execution_count":null,"outputs":[]}]}